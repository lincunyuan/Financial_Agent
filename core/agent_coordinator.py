# ä¸»Agentåè°ƒï¼ˆæµç¨‹æ§åˆ¶ä¸LLMäº¤äº’ï¼‰
from typing import Dict, Optional, List
from datetime import datetime, timedelta
from utils.logging import default_logger as logger
from core.session_manager import RedisSessionManager, build_context_prompt
from core.knowledge_base import FinancialKnowledgeBase
from core.tool_integration import FinancialDataAPI, add_source_citations
from core.llm_client import LLMClient
from utils.config_loader import default_config_loader
from core.chart_generator import ChartGenerator

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from core.session_manager import RedisSessionManager
from core.knowledge_base import FinancialKnowledgeBase
from core.tool_integration import FinancialDataAPI, add_source_citations
from core.llm_client import LLMClient
from utils.config_loader import default_config_loader

# å¯¼å…¥æ–°å¢çš„æ™ºèƒ½æ¨¡å—
from core.intent_recognizer import IntentRecognizer
from core.prompt_engine import PromptEngine
from core.data_processor import DataProcessor
from utils.text_processing import insert_current_time, format_prompt_with_context

# å¯¼å…¥å¸‚åœºæ•°æ®API
from core.mcp import MarketDataAPI, StockDataAPI, NewsAPI, EconomicDataAPI, call_plugin

class FinancialAssistantAgent:
    def __init__(self, config_dir: str = "config"):
        """åˆå§‹åŒ–Agentç»„ä»¶"""
        self.config_loader = default_config_loader
        
        # åŠ è½½æ•°æ®åº“é…ç½®
        db_config = self.config_loader.load_config("database.yaml")
        
        # åˆå§‹åŒ–ä¼šè¯ç®¡ç†å™¨
        redis_host = self.config_loader.get("database.yaml", "redis.host", "localhost")
        redis_port = self.config_loader.get("database.yaml", "redis.port", 6379)
        self.session_manager = RedisSessionManager(host=redis_host, port=redis_port)
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        mysql_config = self.config_loader.get("database.yaml", "mysql", {})
        self.knowledge_base = FinancialKnowledgeBase(
            mysql_host=mysql_config.get("host", "localhost"),
            mysql_user=mysql_config.get("user", "root"),
            mysql_password=mysql_config.get("password", ""),
            mysql_db=mysql_config.get("database", "financial_rag")
        )
        
        # åˆå§‹åŒ–æ•°æ®API
        api_keys_config = self.config_loader.load_config("api_keys.yaml")
        self.data_api = FinancialDataAPI(api_keys=api_keys_config)
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = LLMClient()
        
        # åˆå§‹åŒ–æ„å›¾è¯†åˆ«å™¨
        self.intent_recognizer = IntentRecognizer()
        
        # åˆå§‹åŒ–æ™ºèƒ½æç¤ºè¯æ„å»ºå™¨
        self.prompt_engine = PromptEngine()
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        self.data_processor = DataProcessor()
        
        # åˆå§‹åŒ–å›¾è¡¨ç”Ÿæˆå™¨
        self.chart_generator = ChartGenerator()

    def process_query(self, user_id: str, query: str) -> dict:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´æµç¨‹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            # 1. è·å–å†å²å¯¹è¯
            history = self.session_manager.get_conversation_history(user_id)
            logger.info(f"å†å²å¯¹è¯ç±»å‹: {type(history)}, é•¿åº¦: {len(history)}")
            
            # 2. æ„å›¾è¯†åˆ«å’ŒæŸ¥è¯¢åˆ†æ
            intent_analysis = self._analyze_query_intent(user_id, query, history)
            logger.info(f"æ„å›¾åˆ†æç»“æœç±»å‹: {type(intent_analysis)}, å†…å®¹: {intent_analysis}")
            
            # 2. å¦‚æœæ˜¯ç®€å•æ—¶é—´æŸ¥è¯¢ï¼Œç›´æ¥è¿”å›
            if intent_analysis.get('is_simple_time_query'):
                response = self._handle_simple_time_query(query)
                self.session_manager.store_conversation(user_id, query, response)
                return {
                    "response": response,
                    "intent": "time_query",
                    "user_id": user_id
                }
            
            # ä½¿ç”¨è§£æåçš„æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰ï¼‰
            resolved_query = intent_analysis.get('resolved_query', query)
            logger.info(f"è§£æåçš„æŸ¥è¯¢ï¼š{resolved_query}")
            
            # 3. å‡†å¤‡å†å²å¯¹è¯æ•°æ®
            history_tuples = [(turn['query'], turn['response']) for turn in history]
            
            # 4. æ ¹æ®æ„å›¾è·å–ç›¸å…³æ•°æ®
            relevant_data = self._get_intent_based_data(resolved_query, intent_analysis)
            logger.info(f"ç›¸å…³æ•°æ®ç±»å‹: {type(relevant_data)}, å†…å®¹: {relevant_data}")
            
            # ç¡®ä¿ç›¸å…³æ•°æ®æ˜¯å­—å…¸ç±»å‹
            if not isinstance(relevant_data, dict):
                logger.error(f"ç›¸å…³æ•°æ®ç±»å‹é”™è¯¯: {type(relevant_data)}, å†…å®¹: {relevant_data}")
                relevant_data = {}
            
            # è°ƒè¯•ä¿¡æ¯
            logger.info(f"æ„å›¾åˆ†æç»“æœ: {intent_analysis}")
            logger.info(f"ç›¸å…³æ•°æ®ç»“æ„: {relevant_data}")
            
            # 5. æ„å»ºæ™ºèƒ½æç¤ºè¯
            full_prompt = self._construct_intelligent_prompt(
                resolved_query, history_tuples, relevant_data, intent_analysis
            )
            logger.info(f"æ„å»ºçš„æç¤ºè¯é•¿åº¦: {len(full_prompt)}")
            
            # 6. è°ƒç”¨å¤§æ¨¡å‹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            response = self._call_llm_with_retry(full_prompt, intent_analysis)
            logger.info(f"å¤§æ¨¡å‹å“åº”ç±»å‹: {type(response)}, å†…å®¹: {response}")
            
            # 7. åå¤„ç†å’Œæ¥æºæ ‡æ³¨
            final_response = self._post_process_response(response, relevant_data, intent_analysis)
            
            # 8. å­˜å‚¨å¯¹è¯å†å²
            self.session_manager.store_conversation(user_id, query, final_response)
            
            # 9. å­˜å‚¨æŒ‡ä»£å…³ç³»
            logger.info(f"æ„å›¾åˆ†æç»“æœï¼šresolved_pronouns={intent_analysis.get('resolved_pronouns')}, entities={intent_analysis.get('entities')}")
            
            # å¦‚æœæœ‰è§£æå‡ºçš„ä»£è¯ï¼Œå­˜å‚¨è§£æç»“æœ
            if intent_analysis.get('resolved_pronouns'):
                for resolved in intent_analysis['resolved_pronouns']:
                    logger.info(f"å­˜å‚¨ä»£è¯æŒ‡ä»£å…³ç³»ï¼šuser_id={user_id}, pronoun={resolved.get('pronoun')}, type={resolved.get('type')}, target={resolved.get('target')}, value={resolved.get('value')}")
                    self.session_manager.store_coreference(
                        user_id,
                        pronoun=resolved.get('pronoun'),
                        referent_type=resolved.get('type', 'entity'),
                        referent_target=resolved.get('target', 'stock'),
                        referent_value=resolved.get('value')
                    )
            # å¦‚æœæ²¡æœ‰è§£æå‡ºçš„ä»£è¯ï¼Œä½†è¯†åˆ«åˆ°äº†å®ä½“ï¼Œä¸ºå¯èƒ½çš„åç»­ä»£è¯ï¼ˆå¦‚"å®ƒ"ï¼‰å­˜å‚¨æŒ‡ä»£å…³ç³»
            elif intent_analysis.get('entities'):
                logger.info(f"æ²¡æœ‰è§£æå‡ºä»£è¯ï¼Œä½†è¯†åˆ«åˆ°å®ä½“ï¼š{intent_analysis['entities']}")
                for entity in intent_analysis['entities']:
                    # åŒæ—¶æ£€æŸ¥ä¸¤ç§å¯èƒ½çš„å­—æ®µåï¼š'type'å’Œ'entity_type'
                    entity_type = entity.get('type') or entity.get('entity_type')
                    logger.info(f"å®ä½“ç±»å‹ï¼š{entity_type}")
                    # æ£€æŸ¥å®ä½“ç±»å‹æ˜¯å¦ä¸æˆ‘ä»¬æ”¯æŒçš„ç±»å‹åŒ¹é…
                    if entity_type in ['stock', 'index', 'company', 'stock_name', 'stock_code', 'index_name', 'index_code']:
                        logger.info(f"å®ä½“ç±»å‹åŒ¹é…ï¼š{entity_type}")
                        # ç»Ÿä¸€å®ä½“ç±»å‹ä¸ºå†…éƒ¨ä½¿ç”¨çš„ç±»å‹
                        internal_type = 'stock' if entity_type in ['stock_name', 'stock_code'] else 'index' if entity_type in ['index_name', 'index_code'] else entity_type
                        # è·å–å®ä½“å€¼ï¼Œä¼˜å…ˆä½¿ç”¨valueå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨nameå­—æ®µ
                        entity_value = entity.get('value') or entity.get('name')
                        logger.info(f"å­˜å‚¨ä»£è¯'å®ƒ'çš„æŒ‡ä»£å…³ç³»ï¼šuser_id={user_id}, type={internal_type}, target={entity_type}, value={entity_value}")
                        self.session_manager.store_coreference(
                            user_id,
                            pronoun='å®ƒ',  # ä¸ºæœ€å¸¸ç”¨çš„ä»£è¯åˆ›å»ºæŒ‡ä»£å…³ç³»
                            referent_type=internal_type,
                            referent_target=entity_type,
                            referent_value=entity_value
                        )
                    else:
                        logger.info(f"å®ä½“ç±»å‹ä¸åŒ¹é…ï¼š{entity_type}")
            else:
                logger.info("æ²¡æœ‰è§£æå‡ºä»£è¯ï¼Œä¹Ÿæ²¡æœ‰è¯†åˆ«åˆ°å®ä½“")
            
            # æ„å»ºå“åº”å­—å…¸
            response_dict = {
                "response": final_response,
                "intent": intent_analysis.get('primary_intent', 'general'),
                "entities": intent_analysis.get('entities', []),
                "user_id": user_id
            }
            
            # å¦‚æœæ˜¯Kçº¿å›¾æŸ¥è¯¢ï¼Œæ·»åŠ å›¾è¡¨è·¯å¾„
            if intent_analysis.get('primary_intent') == 'stock_historical_data':
                real_time_data = relevant_data.get('real_time_data', {})
                if "kline_chart" in real_time_data:
                    response_dict["kline_chart"] = real_time_data["kline_chart"]
                if "line_chart" in real_time_data:
                    response_dict["line_chart"] = real_time_data["line_chart"]
                if "volume_chart" in real_time_data:
                    response_dict["volume_chart"] = real_time_data["volume_chart"]
            
            return response_dict
            
        except Exception as e:
            logger.exception(f"å¤„ç†æŸ¥è¯¢å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯: {e}")
            return {
                "response": self._get_fallback_response(query, e),
                "error": str(e),
                "intent": "error",
                "user_id": user_id
            }

    def _analyze_query_intent(self, user_id: str, query: str, history: List[Dict]) -> Dict:
        """æ·±åº¦æ„å›¾è¯†åˆ«å’Œåˆ†æ"""
        # è·å–æŒ‡ä»£å…³ç³»
        coreferences = self.session_manager.get_coreferences(user_id)
        intent_analysis = self.intent_recognizer.analyze(query, history=history, coreferences=coreferences)
        # ç¡®ä¿resolved_queryè¢«åŒ…å«åœ¨æ„å›¾åˆ†æç»“æœä¸­
        if 'resolved_query' not in intent_analysis:
            intent_analysis['resolved_query'] = query
        return intent_analysis

    def _get_intent_based_data(self, query: str, intent_analysis: Dict) -> Dict:
        """æ ¹æ®æ„å›¾è·å–ç›¸å…³æ•°æ®"""
        data_sources = {
            'knowledge_base': [],
            'real_time_data': {},
            'historical_context': {}
        }
        
        intent_type = intent_analysis.get('primary_intent', 'general')
        
        try:
            # 1. æ£€ç´¢çŸ¥è¯†åº“å†…å®¹
            if intent_analysis.get('needs_knowledge_base', True):
                data_sources['knowledge_base'] = self.knowledge_base.retrieve_relevant_chunks(
                    query, 
                    top_k=intent_analysis.get('knowledge_limit', 5)
                )
            
            # 2. è·å–å®æ—¶æ•°æ®
            if intent_analysis.get('needs_real_time_data', False) or intent_analysis.get('primary_intent') == 'stock_historical_data':
                data_sources['real_time_data'] = self._get_enhanced_tool_data(query, intent_analysis)
            
            # 3. è·å–å†å²ä¸Šä¸‹æ–‡
            if intent_analysis.get('needs_historical_context', False):
                data_sources['historical_context'] = self._get_historical_context(intent_analysis)
                
        except Exception as e:
            logger.error(f"è·å–æ„å›¾æ•°æ®å¤±è´¥: {e}")
            data_sources['error'] = str(e)
            
        return data_sources

    def _get_enhanced_tool_data(self, query: str, intent_analysis: Dict) -> Dict:
        """å¢å¼ºçš„å·¥å…·æ•°æ®è·å–ï¼ˆåŸºäºæ„å›¾ï¼‰"""
        tool_data = {}
        intent_type = intent_analysis.get('primary_intent')
        
        try:
            logger.info(f"_get_enhanced_tool_dataè¢«è°ƒç”¨ï¼Œæ„å›¾ç±»å‹ï¼š{intent_type}")
            if intent_type == 'market_news':
                # è·å–è´¢ç»æ–°é—»
                news_query = intent_analysis.get('news_keywords', 'è´¢ç»')
                tool_data["financial_news"] = self.data_api.get_financial_news(
                    query=news_query, 
                    limit=intent_analysis.get('news_limit', 8)
                )
                
            elif intent_type == 'stock_market':
                # è·å–å¸‚åœºæ¦‚å†µ
                tool_data["market_summary"] = self.data_api.get_today_market_summary()
                
                # è·å–ä¸»è¦æŒ‡æ•°
                indices = intent_analysis.get('target_indices', ['ä¸Šè¯æŒ‡æ•°', 'æ·±è¯æˆæŒ‡'])
                for index in indices:
                    tool_data[f"index_{index}"] = self.data_api.get_market_index(index)
                    
            elif intent_type == 'specific_stock':
                # è·å–ç‰¹å®šè‚¡ç¥¨æ•°æ®
                symbols = intent_analysis.get('target_symbols', [])
                if len(symbols) > 0:
                    # å¦‚æœåªæœ‰ä¸€ä¸ªè‚¡ç¥¨ï¼Œä½¿ç”¨å•è‚¡ç¥¨æ ¼å¼
                    if len(symbols) == 1:
                        symbol = symbols[0]
                        stock_data = self.data_api.get_stock_price(symbol)
                        tool_data.update(stock_data)
                    # å¦‚æœæœ‰å¤šä¸ªè‚¡ç¥¨ï¼Œä½¿ç”¨å¤šè‚¡ç¥¨æ ¼å¼
                    else:
                        tool_data['multiple_stocks'] = True
                        tool_data['stock_prices'] = {}
                        for symbol in symbols:
                            stock_data = self.data_api.get_stock_price(symbol)
                            stock_key = stock_data.get('symbol', symbol)
                            tool_data['stock_prices'][stock_key] = stock_data
                    tool_data['source'] = 'akshare_cache' if 'source' in tool_data and tool_data['source'] == 'akshare_cache' else 'akshare'
                    tool_data['timestamp'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
            elif intent_type == 'economic_analysis':
                # è·å–ç»æµæ•°æ®
                indicators = intent_analysis.get('economic_indicators', ['GDP', 'CPI'])
                for indicator in indicators:
                    tool_data[f"economic_{indicator}"] = self.data_api.get_economic_data(indicator)
                    
            elif intent_type == 'stock_historical_data':
                logger.info(f"å¤„ç†stock_historical_dataæ„å›¾ï¼Œæ„å›¾åˆ†æï¼š{intent_analysis}")
                # è·å–å†å²Kçº¿æ•°æ®
                symbols = intent_analysis.get('target_symbols', [])
                logger.info(f"ç›®æ ‡è‚¡ç¥¨ä»£ç ï¼š{symbols}")
                
                # å¦‚æœtarget_symbolsä¸ºç©ºï¼Œå°è¯•ä»entitiesä¸­æå–
                if not symbols:
                    logger.info("target_symbolsä¸ºç©ºï¼Œå°è¯•ä»entitiesä¸­æå–")
                    entities = intent_analysis.get('entities', [])
                    logger.info(f"entitiesï¼š{entities}")
                    
                    for entity in entities:
                        entity_type = entity.get('type')
                        entity_value = entity.get('value')
                        logger.info(f"æ£€æŸ¥å®ä½“ï¼šç±»å‹={entity_type}, å€¼={entity_value}")
                        
                        if entity_type in ['stock_name', 'stock_code']:
                            symbols.append(entity_value)
                        elif entity_type in ['index_name', 'index_code']:
                            symbols.append(entity_value)
                
                logger.info(f"æœ€ç»ˆç›®æ ‡ç¬¦å·ï¼š{symbols}")
                if len(symbols) > 0:
                    symbol = symbols[0]
                    # é»˜è®¤è·å–æœ€è¿‘30å¤©çš„æ•°æ®
                    end_date = datetime.now()
                    start_date = end_date - timedelta(days=30)
                    
                    historical_data = self.data_api.get_historical_data(
                        stock_code=symbol,
                        start_date=start_date,
                        end_date=end_date,
                        interval="1d"
                    )
                    logger.info(f"è·å–çš„å†å²æ•°æ®ï¼š{historical_data}")
                    
                    # æ£€æŸ¥å†å²æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                    logger.info(f"historical_dataå†…å®¹: {historical_data}")
                    # è½¬æ¢ä¸ºDataFrameä»¥ä¾¿ç”Ÿæˆå›¾è¡¨
                    import pandas as pd
                    df = pd.DataFrame()
                    
                    if historical_data.get('data'):
                        df = pd.DataFrame(historical_data['data'])
                        logger.info(f"å†å²æ•°æ®è½¬æ¢ä¸ºDataFrameæˆåŠŸï¼Œè¡Œæ•°: {len(df)}")
                    else:
                        logger.warning(f"historical_dataä¸­æ²¡æœ‰dataå­—æ®µæˆ–dataä¸ºç©º")
                        # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameï¼ŒåŒ…å«å¿…è¦çš„åˆ—
                        df = pd.DataFrame({
                            'æ—¥æœŸ': pd.date_range(end=datetime.now(), periods=10),
                            'å¼€ç›˜': [3400] * 10,
                            'æœ€é«˜': [3410] * 10,
                            'æœ€ä½': [3390] * 10,
                            'æ”¶ç›˜': [3400] * 10,
                            'æˆäº¤é‡': [10000000] * 10
                        })
                        logger.info(f"åˆ›å»ºäº†ä¸€ä¸ªç©ºçš„DataFrameï¼Œè¡Œæ•°: {len(df)}")
                    
                    # å…ˆå°†å†å²æ•°æ®æ›´æ–°åˆ°å·¥å…·æ•°æ®ä¸­
                    tool_data.update(historical_data)
                    
                    try:
                        print(f"\n=== å¼€å§‹ç”Ÿæˆå›¾è¡¨ ===")
                        print(f"è‚¡ç¥¨ä»£ç : {symbol}")
                        print(f"DataFrameå†…å®¹:\n{df}")
                        print(f"DataFrameåˆ—å: {df.columns.tolist()}")
                        
                        # ç”ŸæˆKçº¿å›¾
                        print("\n--- ç”ŸæˆKçº¿å›¾ ---")
                        kline_chart_path = self.chart_generator.generate_k_line_chart(
                            stock_code=symbol,
                            historical_data=df,
                            title=f"{symbol} è¿‘30å¤©Kçº¿å›¾"
                        )
                        print(f"Kçº¿å›¾ç”Ÿæˆç»“æœ: {kline_chart_path}")
                        
                        # ç”Ÿæˆæ”¶ç›˜ä»·æŠ˜çº¿å›¾
                        print("\n--- ç”ŸæˆæŠ˜çº¿å›¾ ---")
                        line_chart_path = self.chart_generator.generate_line_chart(
                            stock_code=symbol,
                            historical_data=df,
                            title=f"{symbol} è¿‘30å¤©æ”¶ç›˜ä»·èµ°åŠ¿"
                        )
                        print(f"æŠ˜çº¿å›¾ç”Ÿæˆç»“æœ: {line_chart_path}")
                        
                        # ç”Ÿæˆæˆäº¤é‡å›¾
                        print("\n--- ç”Ÿæˆæˆäº¤é‡å›¾ ---")
                        volume_chart_path = self.chart_generator.generate_volume_chart(
                            stock_code=symbol,
                            historical_data=df,
                            title=f"{symbol} è¿‘30å¤©æˆäº¤é‡"
                        )
                        print(f"æˆäº¤é‡å›¾ç”Ÿæˆç»“æœ: {volume_chart_path}")
                        
                        # æ·»åŠ å›¾è¡¨è·¯å¾„åˆ°å·¥å…·æ•°æ®ï¼ˆä¼šè¦†ç›–historical_dataä¸­çš„ç©ºå€¼ï¼‰
                        tool_data['kline_chart'] = kline_chart_path
                        tool_data['line_chart'] = line_chart_path
                        tool_data['volume_chart'] = volume_chart_path
                        tool_data['symbol'] = symbol
                        print(f"å›¾è¡¨è·¯å¾„å·²æ·»åŠ åˆ°tool_data: {tool_data}")
                        print("\n=== å›¾è¡¨ç”Ÿæˆå®Œæˆ ===")
                    except Exception as e:
                        logger.error(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
                    logger.info(f"å·¥å…·æ•°æ®æœ€ç»ˆç»“æ„: {tool_data}")
                else:
                    logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç ")
        except Exception as e:
            logger.error(f"è·å–å¢å¼ºå·¥å…·æ•°æ®å¤±è´¥: {e}")
            tool_data['error'] = f"æ•°æ®è·å–å¤±è´¥: {str(e)}"
            
        return tool_data

    def _construct_intelligent_prompt(self, query: str, history: List[tuple], 
                                    data_sources: Dict, intent_analysis: Dict) -> str:
        """æ„å»ºæ™ºèƒ½æç¤ºè¯"""
        # ç¡®ä¿data_sourcesæ˜¯å­—å…¸ç±»å‹
        if not isinstance(data_sources, dict):
            logger.error(f"data_sourcesç±»å‹é”™è¯¯: {type(data_sources)}, å†…å®¹: {data_sources}")
            data_sources = {}
        
        return self.prompt_engine.construct_prompt(
            query=query,
            history=history,
            data_sources=data_sources,
            intent_analysis=intent_analysis
        )

    def _call_llm_with_retry(self, prompt: str, intent_analysis: Dict, max_retries: int = 3) -> str:
        """å¸¦é‡è¯•æœºåˆ¶çš„LLMè°ƒç”¨"""
        for attempt in range(max_retries):
            try:
                response = self.llm_client.generate(prompt)
                
                # éªŒè¯å“åº”è´¨é‡
                if self._validate_response_quality(response, intent_analysis):
                    return response
                else:
                    logger.warning(f"LLMå“åº”è´¨é‡ä¸ä½³ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•")
                    # ä¼˜åŒ–æç¤ºè¯åé‡è¯•
                    prompt = self._enhance_prompt_for_retry(prompt, attempt)
                    
            except Exception as e:
                logger.error(f"LLMè°ƒç”¨å¤±è´¥ï¼ˆç¬¬{attempt+1}æ¬¡ï¼‰: {e}")
                if attempt == max_retries - 1:
                    raise e
                    
        return "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•ç”Ÿæˆæ»¡æ„çš„å›ç­”ã€‚"

    def _validate_response_quality(self, response: str, intent_analysis: Dict) -> bool:
        """éªŒè¯å“åº”è´¨é‡"""
        # åŸºç¡€éªŒè¯
        if not response or len(response.strip()) < 10:
            return False
            
        # æ„å›¾ç‰¹å®šéªŒè¯
        intent_type = intent_analysis.get('primary_intent')
        if intent_type == 'market_news' and 'æ–°é—»' not in response:
            return False
        elif intent_type == 'stock_market' and not any(keyword in response for keyword in ['æ¶¨', 'è·Œ', 'æŒ‡æ•°', 'ç‚¹']):
            return False
            
        return True

    def _post_process_response(self, response: str, data_sources: Dict, intent_analysis: Dict) -> str:
        """å“åº”åå¤„ç†"""
        # 1. æ•°æ®æ•´åˆå’ŒéªŒè¯
        processed_response = self.data_processor.integrate_data_into_response(
            response, data_sources, intent_analysis
        )
        
        # 2. æ·»åŠ å›¾è¡¨ä¿¡æ¯
        intent_type = intent_analysis.get('primary_intent')
        if intent_type == 'stock_historical_data':
            real_time_data = data_sources.get('real_time_data', {})
            if 'kline_chart' in real_time_data and real_time_data['kline_chart']:
                processed_response += f"\n\nğŸ“Š å·²ç”Ÿæˆ{real_time_data.get('symbol')}çš„Kçº¿å›¾ï¼Œæ–‡ä»¶è·¯å¾„ï¼š{real_time_data['kline_chart']}"
            if 'line_chart' in real_time_data and real_time_data['line_chart']:
                processed_response += f"\nğŸ“ˆ å·²ç”Ÿæˆ{real_time_data.get('symbol')}çš„æ”¶ç›˜ä»·èµ°åŠ¿å›¾ï¼Œæ–‡ä»¶è·¯å¾„ï¼š{real_time_data['line_chart']}"
        
        # 3. æ·»åŠ æ¥æºå¼•ç”¨
        final_response = self._add_intelligent_citations(processed_response, data_sources)
        
        # 4. æ ¼å¼ä¼˜åŒ–
        final_response = self._format_response(final_response, intent_analysis)
        
        return final_response

    def _add_intelligent_citations(self, response: str, data_sources: Dict) -> str:
        """æ™ºèƒ½æ·»åŠ æ¥æºå¼•ç”¨"""
        citations = []
        
        # çŸ¥è¯†åº“å¼•ç”¨
        knowledge_chunks = data_sources.get('knowledge_base', [])
        if knowledge_chunks:
            knowledge_citations = []
            for chunk in knowledge_chunks:
                source_info = chunk.get('source', '')
                if source_info and source_info not in knowledge_citations:
                    knowledge_citations.append(source_info)
            
            if knowledge_citations:
                citations.append(f"ğŸ“š å‚è€ƒèµ„æ–™: {', '.join(knowledge_citations)}")
            else:
                citations.append("ğŸ“š çŸ¥è¯†åº“å‚è€ƒ")
            
        # å®æ—¶æ•°æ®å¼•ç”¨
        if data_sources.get('real_time_data'):
            real_time_sources = []
            for key, data in data_sources['real_time_data'].items():
                # ç¡®ä¿dataæ˜¯å­—å…¸ç±»å‹ä¸”åŒ…å«sourceé”®
                if isinstance(data, dict) and data.get('source'):
                    source = data.get('source', 'å®æ—¶æ•°æ®')
                    real_time_sources.append(source)
            
            if real_time_sources:
                citations.append(f"ğŸ“Š å®æ—¶æ•°æ®: {', '.join(set(real_time_sources))}")
        
        if citations:
            return response + "\n\n" + "\n".join(citations)
        
        return response

    def _handle_simple_time_query(self, query: str) -> str:
        """å¤„ç†ç®€å•æ—¶é—´æŸ¥è¯¢"""
        from utils.text_processing import insert_current_time
        return insert_current_time(query)

    def _format_response(self, response: str, intent_analysis: Dict) -> str:
        """æ ¼å¼åŒ–å“åº”è¾“å‡º"""
        # æ ¹æ®ä¸åŒæ„å›¾ç±»å‹è¿›è¡Œå“åº”æ ¼å¼åŒ–
        intent_type = intent_analysis.get('primary_intent')
        
        # å¸‚åœºåˆ†æç±»å“åº”æ ¼å¼
        if intent_type in ['market_news', 'stock_market', 'industry_analysis']:
            response = f"## {intent_type.replace('_', ' ').title()}åˆ†æ\n\n{response}"
        
        # æŠ•èµ„å»ºè®®ç±»å“åº”æ ¼å¼
        elif intent_type in ['investment_advice', 'risk_management']:
            response = f"## {intent_type.replace('_', ' ').title()}\n\nâš ï¸ **é£é™©æç¤º**ï¼šæŠ•èµ„æœ‰é£é™©ï¼Œå†³ç­–éœ€è°¨æ…\n\n{response}"
        
        # è´¢åŠ¡è®¡ç®—ç±»å“åº”æ ¼å¼
        elif intent_type == 'financial_calculation':
            response = f"## è´¢åŠ¡è®¡ç®—ç»“æœ\n\n{response}"
        
        # åŸºç¡€ä¿¡æ¯æŸ¥è¯¢æ ¼å¼
        elif intent_type == 'general':
            response = f"## ä¿¡æ¯æŸ¥è¯¢ç»“æœ\n\n{response}"
        
        return response

    def _get_fallback_response(self, query: str, error: Exception) -> str:
        """é™çº§å“åº”å¤„ç†"""
        # åŸºç¡€ä¿¡æ¯æŸ¥è¯¢
        if any(keyword in query for keyword in ['æ—¶é—´', 'æ—¥æœŸ', 'ä»Šå¤©', 'ç°åœ¨']):
            return self._handle_simple_time_query(query)
            
        # è¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†æŠ€æœ¯é—®é¢˜ã€‚é”™è¯¯è¯¦æƒ…ï¼š{str(error)}"

    def close(self):
        """å…³é—­èµ„æºè¿æ¥"""
        self.knowledge_base.close_connections()