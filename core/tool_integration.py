# å·¥å…·è”åŠ¨ï¼ˆAPIè°ƒç”¨ä¸æ¥æºçº¦æŸï¼‰
import requests
import akshare as ak
import pandas as pd
from typing import Dict, Optional, List, Any
import logging
from datetime import datetime, timedelta
import json
import os
import time
from .prompt_engine import PromptEngine

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FinancialDataAPI:
    def __init__(self, api_keys: Dict[str, Dict] = None, prompt_engine: PromptEngine = None, rag=None):
        """åˆå§‹åŒ–APIå¯†é’¥å’Œé…ç½®"""
        self.api_keys = api_keys or {}
        self.akshare_enabled = True  # é»˜è®¤å¯ç”¨AkShare
        # åˆå§‹åŒ–PromptEngineï¼Œç”¨äºç”Ÿæˆå¢å¼ºå›ç­”
        self.prompt_engine = prompt_engine or PromptEngine(rag=rag)
        self.rag = rag  # ä¿å­˜RAGæ¨¡å—å¼•ç”¨
        
    def analyze_stock_performance(self, symbol: str, days: int = 30) -> Dict[str, Any]:
        """åˆ†æè‚¡ç¥¨è¿‘æœŸè¡¨ç°å¹¶æä¾›å»ºè®®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            days: åˆ†æçš„å¤©æ•°
            
        Returns:
            Dict: åŒ…å«è‚¡ç¥¨è¡¨ç°åˆ†æå’Œå»ºè®®çš„å­—å…¸
        """
        try:
            # è®¡ç®—å¼€å§‹æ—¥æœŸ
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            # è·å–å†å²æ•°æ®
            historical_data = self.get_historical_data(
                symbol, 
                interval="1d",
                start_date=start_date.strftime("%Y-%m-%d"),
                end_date=end_date.strftime("%Y-%m-%d")
            )
            
            if not historical_data or len(historical_data) < 2:
                return {
                    "performance": None,
                    "analysis": "æ— æ³•è·å–è¶³å¤Ÿçš„å†å²æ•°æ®è¿›è¡Œåˆ†æ",
                    "advice": "å»ºè®®å…³æ³¨åç»­èµ°åŠ¿"
                }
            
            # è®¡ç®—åŒºé—´è¡¨ç°
            first_close = historical_data[0]['close']
            last_close = historical_data[-1]['close']
            price_change = last_close - first_close
            price_change_percent = (price_change / first_close) * 100
            
            # è®¡ç®—æœ€é«˜ä»·å’Œæœ€ä½ä»·
            highs = [data['high'] for data in historical_data]
            lows = [data['low'] for data in historical_data]
            max_price = max(highs)
            min_price = min(lows)
            
            # è®¡ç®—æ³¢åŠ¨ç‡
            volatility = (max_price - min_price) / first_close * 100
            
            # è®¡ç®—æˆäº¤é‡æƒ…å†µ
            volumes = [data['volume'] for data in historical_data]
            avg_volume = sum(volumes) / len(volumes)
            recent_volume = sum(volumes[-5:]) / 5 if len(volumes) >= 5 else avg_volume
            volume_change = (recent_volume - avg_volume) / avg_volume * 100
            
            # è®¡ç®—æ”¯æ’‘ä½å’Œé˜»åŠ›ä½
            support_levels = []
            resistance_levels = []
            
            # 1. ç§»åŠ¨å¹³å‡çº¿ä½œä¸ºæ”¯æ’‘/é˜»åŠ›ä½
            closes = [data['close'] for data in historical_data]
            if len(closes) >= 5:
                ma5 = sum(closes[-5:]) / 5
                support_levels.append(round(ma5, 2))
            if len(closes) >= 10:
                ma10 = sum(closes[-10:]) / 10
                support_levels.append(round(ma10, 2))
            if len(closes) >= 20:
                ma20 = sum(closes[-20:]) / 20
                support_levels.append(round(ma20, 2))
            
            # 2. æœ€è¿‘çš„ä½ç‚¹ä½œä¸ºæ”¯æ’‘ä½ï¼Œæœ€è¿‘çš„é«˜ç‚¹ä½œä¸ºé˜»åŠ›ä½
            recent_lows = sorted(lows[-20:])[:3]  # æœ€è¿‘20å¤©çš„3ä¸ªä½ç‚¹
            recent_highs = sorted(highs[-20:], reverse=True)[:3]  # æœ€è¿‘20å¤©çš„3ä¸ªé«˜ç‚¹
            
            support_levels.extend([round(low, 2) for low in recent_lows])
            resistance_levels.extend([round(high, 2) for high in recent_highs])
            
            # å»é‡å¹¶æ’åº
            support_levels = sorted(list(set(support_levels)))
            resistance_levels = sorted(list(set(resistance_levels)), reverse=True)
            
            # ç”Ÿæˆåˆ†æå’Œå»ºè®®
            analysis = []
            advice = []
            
            if price_change_percent > 0:
                analysis.append(f"æœ€è¿‘{days}å¤©ä¸Šæ¶¨äº†{price_change_percent:.2f}%")
                if price_change_percent > 10:
                    analysis.append("è¡¨ç°å¼ºåŠ²ï¼Œæ¶¨å¹…è¶…è¿‡10%")
                    if resistance_levels:
                        advice.append(f"å…³æ³¨é˜»åŠ›ä½{resistance_levels[0]}å…ƒï¼Œè‹¥çªç ´è€ƒè™‘ç»§ç»­æŒæœ‰")
                    else:
                        advice.append("çŸ­æœŸå¯èƒ½å­˜åœ¨å›è°ƒé£é™©ï¼Œå»ºè®®è°¨æ…æŒæœ‰æˆ–éƒ¨åˆ†å‡ä»“")
                else:
                    analysis.append("è¡¨ç°ç¨³å¥")
                    advice.append("å»ºè®®ç»§ç»­æŒæœ‰ï¼Œå…³æ³¨åç»­æˆäº¤é‡å˜åŒ–")
            else:
                analysis.append(f"æœ€è¿‘{days}å¤©ä¸‹è·Œäº†{-price_change_percent:.2f}%")
                if price_change_percent < -10:
                    analysis.append("è·Œå¹…è¾ƒå¤§ï¼Œè¶…è¿‡10%")
                    if support_levels:
                        advice.append(f"å…³æ³¨æ”¯æ’‘ä½{support_levels[-1]}å…ƒï¼Œè‹¥è·Œç ´è€ƒè™‘æ­¢æŸ")
                    else:
                        advice.append("å»ºè®®å…³æ³¨æ”¯æ’‘ä½ï¼Œè‹¥è·Œç ´æ”¯æ’‘ä½è€ƒè™‘æ­¢æŸ")
                else:
                    analysis.append("å›è°ƒå¹…åº¦åœ¨åˆç†èŒƒå›´å†…")
                    advice.append("å»ºè®®å…³æ³¨åŸºæœ¬é¢å˜åŒ–ï¼Œç­‰å¾…ä¼ç¨³ä¿¡å·")
            
            if volatility > 20:
                analysis.append("è‚¡ä»·æ³¢åŠ¨è¾ƒå¤§ï¼Œé£é™©è¾ƒé«˜")
                advice.append("å»ºè®®æ§åˆ¶ä»“ä½ï¼Œé¿å…è¿½é«˜")
            else:
                analysis.append("è‚¡ä»·æ³¢åŠ¨ç›¸å¯¹ç¨³å®š")
                if support_levels:
                    advice.append(f"é£é™©å¯æ§ï¼Œå¯ä»¥è€ƒè™‘åœ¨æ”¯æ’‘ä½{support_levels[-1]}å…ƒé™„è¿‘é€¢ä½å¸ƒå±€")
                else:
                    advice.append("é£é™©å¯æ§ï¼Œå¯ä»¥è€ƒè™‘é€¢ä½å¸ƒå±€")
            
            if volume_change > 30:
                analysis.append("è¿‘æœŸæˆäº¤é‡æ˜æ˜¾æ”¾å¤§")
                advice.append("æˆäº¤é‡æ”¾å¤§å¯èƒ½é¢„ç¤ºç€è¡Œæƒ…å˜åŒ–ï¼Œå»ºè®®å¯†åˆ‡å…³æ³¨")
            elif volume_change < -30:
                analysis.append("è¿‘æœŸæˆäº¤é‡æ˜æ˜¾èç¼©")
                advice.append("æˆäº¤é‡èç¼©å¯èƒ½æ„å‘³ç€ç¼ºä¹èµ„é‡‘å…³æ³¨ï¼Œå»ºè®®è°¨æ…")
            
            # æ„å»ºè¡¨ç°åˆ†æç»“æœ
            performance = {
                "start_date": start_date.strftime("%Y-%m-%d"),
                "end_date": end_date.strftime("%Y-%m-%d"),
                "period_days": days,
                "start_price": first_close,
                "end_price": last_close,
                "price_change": price_change,
                "price_change_percent": price_change_percent,
                "max_price": max_price,
                "min_price": min_price,
                "volatility": volatility,
                "avg_volume": avg_volume,
                "recent_volume": recent_volume,
                "volume_change_percent": volume_change,
                "support_levels": support_levels,
                "resistance_levels": resistance_levels
            }
            
            return {
                "performance": performance,
                "analysis": "ï¼Œ".join(analysis),
                "advice": "ã€‚".join(advice)
            }
            
        except Exception as e:
            logger.error(f"åˆ†æè‚¡ç¥¨è¡¨ç°å¤±è´¥ {symbol}: {str(e)}")
            return {
                "performance": None,
                "analysis": "è‚¡ç¥¨è¡¨ç°åˆ†æå¤±è´¥",
                "advice": "å»ºè®®ç»“åˆå…¶ä»–ä¿¡æ¯è¿›è¡ŒæŠ•èµ„å†³ç­–"
            }
    
    def get_stock_price(self, symbol: str, time: Optional[datetime] = None) -> Optional[Dict]:
        """è·å–è‚¡ç¥¨å®æ—¶ä»·æ ¼ï¼ˆä¼˜å…ˆä½¿ç”¨APIæŸ¥è¯¢ï¼Œå¤±è´¥æ—¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰ï¼Œå¹¶æä¾›è¿‘æœŸè¡¨ç°åˆ†æçš„å»ºè®®"""
        try:
            # ä¼˜å…ˆä»AkShare APIè·å–æœ€æ–°æ•°æ®
            ak_data = self._get_stock_price_akshare(symbol)
            if ak_data:
                # åˆ†æè¿‘æœŸè¡¨ç°
                performance = self.analyze_stock_performance(symbol)
                ak_data["performance_analysis"] = performance
                return ak_data
            
            # AkShareå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜
            cache_data = self._get_stock_price_from_cache(symbol)
            if cache_data:
                # åˆ†æè¿‘æœŸè¡¨ç°
                performance = self.analyze_stock_performance(symbol)
                cache_data["performance_analysis"] = performance
                return cache_data
            
            # ç¼“å­˜è·å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Alpha Vantage
            alpha_data = self._get_stock_price_alpha_vantage(symbol)
            if alpha_data:
                # åˆ†æè¿‘æœŸè¡¨ç°
                performance = self.analyze_stock_performance(symbol)
                alpha_data["performance_analysis"] = performance
                return alpha_data
            
            # éƒ½å¤±è´¥äº†ï¼Œè¿”å›None
            return None
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨ä»·æ ¼å¤±è´¥ {symbol}: {str(e)}")
            return None
    
    def _get_stock_price_from_cache(self, symbol: str) -> Optional[Dict]:
        """ä»æœ¬åœ°ç¼“å­˜æ–‡ä»¶è·å–è‚¡ç¥¨ä»·æ ¼ï¼ˆä¼˜å…ˆCSVç¼“å­˜ï¼Œå…¶æ¬¡JSONç¼“å­˜ï¼‰"""
        try:
            # æå–è‚¡ç¥¨ä»£ç ï¼ˆå¤„ç†å¤šç§æ ¼å¼ï¼š601288.SS, sh601288, 601288ï¼‰
            stock_code = symbol.split('.')[0]  # å»æ‰åç¼€
            # å»æ‰å‰ç¼€ï¼ˆå¦‚sh, szï¼‰
            if stock_code.startswith('sh') or stock_code.startswith('sz'):
                stock_code = stock_code[2:]
            elif stock_code.startswith('SSE') or stock_code.startswith('SZSE'):
                stock_code = stock_code[3:]
            
            # æ­¥éª¤1ï¼šå°è¯•ä»stock_dataç›®å½•ä¸‹çš„CSVæ–‡ä»¶è·å–
            stock_data_dir = r"D:\code\financial_assistant_agent\stock_data"
            csv_files = [f for f in os.listdir(stock_data_dir) if f.endswith('.csv')]
            if csv_files:
                # æŒ‰æ—¥æœŸæ’åºï¼Œè·å–æœ€æ–°çš„CSVæ–‡ä»¶
                csv_files.sort(reverse=True)
                latest_csv = os.path.join(stock_data_dir, csv_files[0])
                logger.info(f"æ­£åœ¨ä»ç¼“å­˜æ–‡ä»¶è·å–è‚¡ç¥¨æ•°æ®: {latest_csv}")
                
                # è¯»å–CSVæ–‡ä»¶ï¼ˆä½¿ç”¨æ­£ç¡®çš„ç¼–ç ï¼‰
                df = pd.read_csv(latest_csv, encoding='utf-8')
                
                # æŸ¥æ‰¾åŒ¹é…çš„è‚¡ç¥¨æ•°æ®ï¼ˆä»£ç åˆ—å¯èƒ½æ˜¯"ä»£ç "æˆ–"code"ï¼‰
                code_column = "ä»£ç " if "ä»£ç " in df.columns else "code" if "code" in df.columns else None
                if code_column:
                    # ç¡®ä¿è‚¡ç¥¨ä»£ç åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
                    df[code_column] = df[code_column].astype(str)
                    
                    # åŒ¹é…è‚¡ç¥¨ä»£ç 
                    stock_data = df[df[code_column] == stock_code]
                    if not stock_data.empty:
                        # è·å–ç¬¬ä¸€è¡ŒåŒ¹é…çš„æ•°æ®
                        stock_data = stock_data.iloc[0]
                        
                        # è¿”å›å®Œæ•´çš„æ•°æ®ç»“æ„
                        return {
                            'symbol': symbol,
                            'price': float(stock_data["æœ€æ–°ä»·"]),
                            'change': float(stock_data["æ¶¨è·Œå¹…"]),
                            'change_amount': float(stock_data["æ¶¨è·Œé¢"]),
                            'volume': int(stock_data["æˆäº¤é‡"]),
                            'amount': float(stock_data["æˆäº¤é¢"]),
                            'high': float(stock_data["æœ€é«˜"]),
                            'low': float(stock_data["æœ€ä½"]),
                            'open': float(stock_data["ä»Šå¼€"]),
                            'prev_close': float(stock_data["æ˜¨æ”¶"]),
                            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'source': 'akshare_csv_cache'
                        }
            
            # æ­¥éª¤2ï¼šå°è¯•ä»cache/stock_dataç›®å½•ä¸‹çš„JSONç¼“å­˜æ–‡ä»¶è·å–
            cache_dir = r"D:\code\financial_assistant_agent\cache\stock_data"
            logger.info(f"å°è¯•ä»JSONç¼“å­˜ç›®å½•è·å–æ•°æ®: {cache_dir}")
            
            if not os.path.exists(cache_dir):
                logger.warning(f"JSONç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_dir}")
                return None
            
            # æŸ¥æ‰¾åŒ¹é…çš„JSONç¼“å­˜æ–‡ä»¶ï¼ˆå¦‚600519.SS_1d.jsonæˆ–sh601288_1d.jsonï¼‰
            json_files = []
            for file in os.listdir(cache_dir):
                if file.endswith('.json'):
                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«è‚¡ç¥¨ä»£ç ï¼ˆå¯èƒ½å¸¦æˆ–ä¸å¸¦å‰ç¼€/åç¼€ï¼‰
                    if stock_code in file or symbol in file:
                        json_files.append(os.path.join(cache_dir, file))
            
            if not json_files:
                logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„JSONç¼“å­˜æ–‡ä»¶ï¼Œè‚¡ç¥¨ä»£ç : {stock_code}ï¼ŒåŸå§‹ç¬¦å·: {symbol}")
                return None
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„JSONç¼“å­˜æ–‡ä»¶
            json_files.sort(key=os.path.getmtime, reverse=True)
            latest_json = json_files[0]
            logger.info(f"æ­£åœ¨ä»JSONç¼“å­˜æ–‡ä»¶è·å–è‚¡ç¥¨æ•°æ®: {latest_json}")
            
            # è¯»å–JSONç¼“å­˜æ–‡ä»¶
            with open(latest_json, 'r', encoding='utf-8') as f:
                historical_data = json.load(f)
            
            if not historical_data:
                logger.warning(f"JSONç¼“å­˜æ–‡ä»¶ä¸ºç©º: {latest_json}")
                return None
            
            # æŒ‰æ—¥æœŸæ’åºï¼Œè·å–æœ€æ–°çš„ä¸€æ¡æ•°æ®
            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®ï¼Œä»¥ä¾¿æ’åº
            def parse_date(item):
                if 'date' in item:
                    return datetime.strptime(item['date'], '%Y-%m-%d')
                return datetime.min
            
            historical_data.sort(key=parse_date, reverse=True)
            latest_data = historical_data[0]
            
            # ä»æœ€æ–°æ•°æ®ä¸­æå–æ‰€éœ€å­—æ®µ
            # è®¡ç®—æ¶¨è·Œå¹…å’Œæ¶¨è·Œé¢
            close_price = float(latest_data.get('close', 0))
            prev_close = float(latest_data.get('prev_close', close_price))  # å¦‚æœæ²¡æœ‰æ˜¨æ”¶ä»·ï¼Œä½¿ç”¨æ”¶ç›˜ä»·
            change_amount = close_price - prev_close
            change_percent = (change_amount / prev_close) * 100 if prev_close != 0 else 0
            
            # è¿”å›å®Œæ•´çš„æ•°æ®ç»“æ„
            return {
                'symbol': symbol,
                'price': close_price,
                'change': change_percent,
                'change_amount': change_amount,
                'volume': int(latest_data.get('volume', 0)),
                'amount': float(latest_data.get('amount', 0)),
                'high': float(latest_data.get('high', 0)),
                'low': float(latest_data.get('low', 0)),
                'open': float(latest_data.get('open', 0)),
                'prev_close': prev_close,
                'timestamp': latest_data.get('date', datetime.now().strftime('%Y-%m-%d')),
                'source': 'akshare_json_cache'
            }
            
        except Exception as e:
            logger.error(f"ä»ç¼“å­˜è·å–è‚¡ç¥¨ä»·æ ¼å¤±è´¥ {symbol}: {str(e)}")
            return None

    def _get_stock_price_akshare(self, symbol: str) -> Optional[Dict]:
        """ä½¿ç”¨AkShareè·å–è‚¡ç¥¨æ•°æ®"""
        try:
            if symbol.endswith('.SS') or symbol.endswith('.SZ'):
                # Aè‚¡æ•°æ®
                stock_code = symbol.replace('.SS', '').replace('.SZ', '')
                # å»æ‰å‰ç¼€ï¼ˆå¦‚sh, szï¼‰
                if stock_code.startswith('sh') or stock_code.startswith('sz'):
                    stock_code = stock_code[2:]
                elif stock_code.startswith('SSE') or stock_code.startswith('SZSE'):
                    stock_code = stock_code[3:]
                
                # æ­¥éª¤1ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å½“å¤©çš„ç¼“å­˜æ–‡ä»¶ä¸”ä¸è¶…è¿‡30åˆ†é’Ÿ
                today = datetime.now().strftime('%Y%m%d')
                cache_file = f"stock_data/akshare_stock_data_{today}.csv"
                
                use_cache = False
                if os.path.exists(cache_file):
                    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
                    file_time = os.path.getmtime(cache_file)
                    current_time = time.time()
                    # å¦‚æœç¼“å­˜æ–‡ä»¶ä¸è¶…è¿‡30åˆ†é’Ÿï¼Œä½¿ç”¨ç¼“å­˜
                    if (current_time - file_time) < 1800:  # 30åˆ†é’Ÿ = 1800ç§’
                        logger.info(f"ä½¿ç”¨å½“å¤©ç¼“å­˜æ–‡ä»¶: {cache_file}")
                        # ä»ç¼“å­˜æ–‡ä»¶è¯»å–æ•°æ®
                        data = pd.read_csv(cache_file)
                        use_cache = True
                    else:
                        logger.info(f"ç¼“å­˜æ–‡ä»¶è¶…è¿‡30åˆ†é’Ÿï¼Œé‡æ–°è·å–æ•°æ®: {cache_file}")
                
                if not use_cache:
                    logger.info("æ²¡æœ‰æœ‰æ•ˆç¼“å­˜æ–‡ä»¶ï¼Œè°ƒç”¨AkShare APIè·å–æ•°æ®")
                    # è°ƒç”¨APIè·å–æ•°æ®
                    data = ak.stock_zh_a_spot_em()
                    # ä¿å­˜ä¸ºå½“å¤©çš„ç¼“å­˜æ–‡ä»¶
                    data.to_csv(cache_file, index=False, encoding='utf-8')
                    logger.info(f"å·²ä¿å­˜å½“å¤©æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}")
                    
                stock_data = data[data['ä»£ç '] == stock_code]
                
                if not stock_data.empty:
                    # è·å–æ•°æ®è¡Œ
                    row = stock_data.iloc[0]
                    
                    # æ„å»ºè¿”å›ç»“æœï¼Œä½¿ç”¨try-exceptå¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
                    result = {
                        'symbol': symbol,
                        'price': float(row['æœ€æ–°ä»·']),
                        'change': float(row['æ¶¨è·Œå¹…']),
                        'change_amount': float(row['æ¶¨è·Œé¢']),
                        'volume': int(row['æˆäº¤é‡']),
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'source': 'akshare_daily_cache' if use_cache else 'akshare'
                    }
                    
                    # å°è¯•è·å–å…¶ä»–å¯é€‰å­—æ®µ
                    try:
                        result['amount'] = float(row['æˆäº¤é¢'])
                    except (KeyError, ValueError, TypeError):
                        result['amount'] = 0.0
                        
                    try:
                        result['high'] = float(row['æœ€é«˜ä»·'] if 'æœ€é«˜ä»·' in row else row['æœ€é«˜'])
                    except (KeyError, ValueError, TypeError):
                        result['high'] = float(row['æœ€æ–°ä»·'])
                        
                    try:
                        result['low'] = float(row['æœ€ä½ä»·'] if 'æœ€ä½ä»·' in row else row['æœ€ä½'])
                    except (KeyError, ValueError, TypeError):
                        result['low'] = float(row['æœ€æ–°ä»·'])
                        
                    try:
                        result['open'] = float(row['ä»Šå¼€'])
                    except (KeyError, ValueError, TypeError):
                        result['open'] = float(row['æœ€æ–°ä»·'])
                        
                    try:
                        result['prev_close'] = float(row['æ˜¨æ”¶'])
                    except (KeyError, ValueError, TypeError):
                        result['prev_close'] = float(row['æœ€æ–°ä»·'])
                    
                    return result
                    
            elif symbol.endswith('.HK'):
                # æ¸¯è‚¡æ•°æ®
                stock_code = symbol.replace('.HK', '')
                
                # æ­¥éª¤1ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å½“å¤©çš„ç¼“å­˜æ–‡ä»¶
                today = datetime.now().strftime('%Y%m%d')
                cache_file = f"stock_data/akshare_hk_stock_data_{today}.csv"
                
                if os.path.exists(cache_file):
                    logger.info(f"ä½¿ç”¨å½“å¤©æ¸¯è‚¡ç¼“å­˜æ–‡ä»¶: {cache_file}")
                    # ä»ç¼“å­˜æ–‡ä»¶è¯»å–æ•°æ®
                    data = pd.read_csv(cache_file)
                else:
                    logger.info("æ²¡æœ‰å½“å¤©æ¸¯è‚¡ç¼“å­˜æ–‡ä»¶ï¼Œè°ƒç”¨AkShare APIè·å–æ•°æ®")
                    # è°ƒç”¨APIè·å–æ•°æ®
                    data = ak.stock_hk_spot_em()
                    # ä¿å­˜ä¸ºå½“å¤©çš„ç¼“å­˜æ–‡ä»¶
                    data.to_csv(cache_file, index=False, encoding='utf-8')
                    logger.info(f"å·²ä¿å­˜å½“å¤©æ¸¯è‚¡æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}")
                
                stock_data = data[data['ä»£ç '] == stock_code]
                
                if not stock_data.empty:
                    # è·å–æ•°æ®è¡Œ
                    row = stock_data.iloc[0]
                    
                    result = {
                        'symbol': symbol,
                        'price': float(row['æœ€æ–°ä»·']),
                        'change': float(row['æ¶¨è·Œå¹…']),
                        'change_amount': float(row['æ¶¨è·Œé¢']),
                        'volume': int(row['æˆäº¤é‡']),
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'source': 'akshare' if not os.path.exists(cache_file) else 'akshare_daily_cache'
                    }
                    
                    # å°è¯•è·å–å…¶ä»–å¯é€‰å­—æ®µ
                    try:
                        result['amount'] = float(row['æˆäº¤é¢'])
                    except (KeyError, ValueError, TypeError):
                        result['amount'] = 0.0
                        
                    return result
                    
        except Exception as e:
            logger.error(f"ä½¿ç”¨AkShareè·å–è‚¡ç¥¨ä»·æ ¼å¤±è´¥ {symbol}: {str(e)}")
            return None

    def _get_stock_price_alpha_vantage(self, symbol: str) -> Optional[Dict]:
        """ä½¿ç”¨Alpha Vantageè·å–è‚¡ç¥¨æ•°æ®ï¼ˆä¸»è¦ç¾è‚¡ï¼‰"""
        try:
            if not self.api_keys.get("alpha_vantage"):
                return None
                
            alpha_config = self.api_keys["alpha_vantage"]
            params = {
                "function": "GLOBAL_QUOTE",
                "symbol": symbol.replace('.US', ''),
                "apikey": alpha_config["api_key"]
            }
            
            response = requests.get(alpha_config["base_url"], params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if "Global Quote" in data:
                quote = data["Global Quote"]
                return {
                    'symbol': symbol,
                    'price': float(quote.get("05. price", 0)),
                    'change': float(quote.get("10. change percent", "0%").replace('%', '')),
                    'change_amount': float(quote.get("09. change", 0)),
                    'volume': int(quote.get("06. volume", 0)),
                    'timestamp': quote.get("07. latest trading day", ""),
                    'source': 'alpha_vantage'
                }
                
        except Exception as e:
            logger.warning(f"Alpha Vantageè·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
            
        return None

    def get_market_index(self, index_name: str, time: Optional[datetime] = None) -> Optional[Dict]:
        """è·å–å¸‚åœºæŒ‡æ•°æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨AkShareï¼‰"""
        try:
            # ä½¿ç”¨AkShareè·å–Aè‚¡æŒ‡æ•°
            akshare_result = self._get_market_index_akshare(index_name)
            if akshare_result:
                return akshare_result
                
            # å¤‡ç”¨æ–¹æ¡ˆï¼šTwelveData
            return self._get_market_index_twelvedata(index_name)
        except Exception as e:
            logger.error(f"è·å–æŒ‡æ•°æ•°æ®å¤±è´¥: {e}")
            return None

    def _get_market_index_akshare(self, index_name: str) -> Optional[Dict]:
        """ä½¿ç”¨AkShareè·å–æŒ‡æ•°æ•°æ®"""
        try:
            # è·å–Aè‚¡æŒ‡æ•°å®æ—¶æ•°æ®
            data = ak.stock_zh_index_spot()
            
            # æŒ‡æ•°ä»£ç æ˜ å°„
            index_mapping = {
                'ä¸Šè¯æŒ‡æ•°': '000001',
                'æ·±è¯æˆæŒ‡': '399001',
                'åˆ›ä¸šæ¿æŒ‡': '399006',
                'æ²ªæ·±300': '000300',
                'ä¸Šè¯50': '000016'
            }
            
            if index_name in index_mapping:
                index_code = index_mapping[index_name]
                index_data = data[data['ä»£ç '] == index_code]
                
                if not index_data.empty:
                    return {
                        'name': index_name,
                        'price': float(index_data.iloc[0]['æœ€æ–°ä»·']),
                        'change': float(index_data.iloc[0]['æ¶¨è·Œå¹…']),
                        'change_amount': float(index_data.iloc[0]['æ¶¨è·Œé¢']),
                        'volume': int(index_data.iloc[0]['æˆäº¤é‡']),
                        'amount': float(index_data.iloc[0]['æˆäº¤é¢']),
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'source': 'akshare'
                    }
                    
        except Exception as e:
            logger.warning(f"AkShareè·å–æŒ‡æ•°æ•°æ®å¤±è´¥: {e}")
            
        return None

    def _get_market_index_twelvedata(self, index_name: str) -> Optional[Dict]:
        """ä½¿ç”¨TwelveDataè·å–æŒ‡æ•°æ•°æ®"""
        try:
            if not self.api_keys.get("twelvedata"):
                return None
                
            twelve_config = self.api_keys["twelvedata"]
            params = {
                "symbol": index_name,
                "apikey": twelve_config["api_key"]
            }
            
            response = requests.get(f"{twelve_config['base_url']}/price", params=params, timeout=10)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.warning(f"TwelveDataè·å–æŒ‡æ•°æ•°æ®å¤±è´¥: {e}")
            return None

    def get_financial_news(self, query: str = "", limit: int = 10) -> Optional[Dict]:
        """è·å–è´¢ç»æ–°é—»ï¼ˆå¤šæ•°æ®æºï¼‰"""
        try:
            # ä¼˜å…ˆä½¿ç”¨AkShareè·å–è´¢ç»æ–°é—»
            akshare_news = self._get_news_akshare(limit)
            
            # å¤‡ç”¨ï¼šNewsAPI
            newsapi_news = self._get_news_newsapi(query, limit)
            
            # åˆå¹¶ç»“æœ
            all_articles = []
            if akshare_news and 'articles' in akshare_news:
                all_articles.extend(akshare_news['articles'])
            if newsapi_news and 'articles' in newsapi_news:
                all_articles.extend(newsapi_news['articles'])
                
            return {
                'articles': all_articles[:limit],
                'total': len(all_articles),
                'timestamp': datetime.now().isoformat(),
                'sources': ['akshare', 'newsapi']
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}")
            return None

    def _get_news_akshare(self, limit: int = 10) -> Optional[Dict]:
        """ä½¿ç”¨AkShareè·å–è´¢ç»æ–°é—»"""
        try:
            # è·å–è´¢ç»æ–°é—»
            news_data = ak.news_roll()
            
            articles = []
            for index, row in news_data.head(limit).iterrows():
                article = {
                    'title': row['æ–°é—»æ ‡é¢˜'],
                    'description': row['æ–°é—»å†…å®¹'][:200] + '...' if len(str(row['æ–°é—»å†…å®¹'])) > 200 else str(row['æ–°é—»å†…å®¹']),
                    'source': row['æ–°é—»æ¥æº'],
                    'publishedAt': row['å‘å¸ƒæ—¶é—´'],
                    'url': row['æ–°é—»é“¾æ¥']
                }
                articles.append(article)
                
            return {'articles': articles}
            
        except Exception as e:
            logger.warning(f"AkShareè·å–æ–°é—»å¤±è´¥: {e}")
            return None

    def _get_news_newsapi(self, query: str = "", limit: int = 5) -> Optional[Dict]:
        """ä½¿ç”¨NewsAPIè·å–è´¢ç»æ–°é—»"""
        try:
            if not self.api_keys.get("newsapi"):
                return None
                
            news_config = self.api_keys["newsapi"]
            params = {
                "q": query or "è´¢ç» è‚¡ç¥¨ ç»æµ",
                "language": "zh",
                "sortBy": "publishedAt",
                "pageSize": limit,
                "apiKey": news_config["api_key"]
            }
            
            response = requests.get(f"{news_config['base_url']}/everything", params=params, timeout=15)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.warning(f"NewsAPIè·å–æ–°é—»å¤±è´¥: {e}")
            return None

    def get_today_market_summary(self) -> Optional[Dict]:
        """è·å–ä»Šæ—¥å¸‚åœºæ¦‚å†µ"""
        try:
            summary = {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'major_indices': {},
                'market_activity': {},
                'hot_sectors': []
            }
            
            # è·å–ä¸»è¦æŒ‡æ•°
            indices = ['ä¸Šè¯æŒ‡æ•°', 'æ·±è¯æˆæŒ‡', 'åˆ›ä¸šæ¿æŒ‡', 'æ²ªæ·±300']
            for index in indices:
                index_data = self.get_market_index(index)
                if index_data:
                    summary['major_indices'][index] = index_data
            
            # è·å–å¸‚åœºæ´»è·ƒåº¦
            try:
                market_activity = ak.stock_market_activity()
                summary['market_activity'] = {
                    'total_companies': market_activity.get('æ€»æ•°', 'N/A'),
                    'rising_companies': market_activity.get('ä¸Šæ¶¨å®¶æ•°', 'N/A'),
                    'falling_companies': market_activity.get('ä¸‹è·Œå®¶æ•°', 'N/A'),
                    'unchanged_companies': market_activity.get('å¹³ç›˜å®¶æ•°', 'N/A')
                }
            except Exception as e:
                logger.warning(f"è·å–å¸‚åœºæ´»è·ƒåº¦å¤±è´¥: {e}")
            
            # è·å–çƒ­é—¨æ¿å—
            try:
                hot_sectors = ak.stock_board_concept_spot_em()
                summary['hot_sectors'] = hot_sectors.head(5).to_dict('records')
            except Exception as e:
                logger.warning(f"è·å–çƒ­é—¨æ¿å—å¤±è´¥: {e}")
            
            return summary
            
        except Exception as e:
            logger.error(f"è·å–å¸‚åœºæ¦‚å†µå¤±è´¥: {e}")
            return None

    def get_stock_intraday(self, symbol: str, interval: str = "5min") -> Optional[Dict]:
        """è·å–è‚¡ç¥¨æ—¥å†…æ•°æ®"""
        try:
            # ä¼˜å…ˆä½¿ç”¨AkShare
            if symbol.endswith('.SS') or symbol.endswith('.SZ'):
                stock_code = symbol.replace('.SS', '').replace('.SZ', '')
                data = ak.stock_zh_a_hist_min_em(symbol=stock_code, period=interval)
                
                if not data.empty:
                    return {
                        'symbol': symbol,
                        'data': data.to_dict('records'),
                        'interval': interval,
                        'source': 'akshare'
                    }
                    
            # å¤‡ç”¨ï¼šAlpha Vantage
            return self._get_stock_intraday_alpha_vantage(symbol, interval)
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨æ—¥å†…æ•°æ®å¤±è´¥: {e}")
            return None

    def _get_stock_intraday_alpha_vantage(self, symbol: str, interval: str) -> Optional[Dict]:
        """ä½¿ç”¨Alpha Vantageè·å–æ—¥å†…æ•°æ®"""
        try:
            if not self.api_keys.get("alpha_vantage"):
                return None
                
            alpha_config = self.api_keys["alpha_vantage"]
            params = {
                "function": "TIME_SERIES_INTRADAY",
                "symbol": symbol.replace('.US', ''),
                "interval": interval,
                "apikey": alpha_config["api_key"]
            }
            
            response = requests.get(alpha_config["base_url"], params=params, timeout=10)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.warning(f"Alpha Vantageè·å–æ—¥å†…æ•°æ®å¤±è´¥: {e}")
            return None

    def get_economic_data(self, indicator: str = "GDP") -> Optional[Dict]:
        """è·å–ç»æµæ•°æ®"""
        try:
            # ä½¿ç”¨AkShareè·å–å®è§‚ç»æµæ•°æ®
            if indicator == "GDP":
                data = ak.macro_china_gdp()
            elif indicator == "CPI":
                data = ak.macro_china_cpi()
            elif indicator == "PPI":
                data = ak.macro_china_ppi()
            else:
                return None
                
            return {
                'indicator': indicator,
                'data': data.to_dict('records'),
                'source': 'akshare',
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–ç»æµæ•°æ®å¤±è´¥: {e}")
            return None

    def get_historical_data(self, symbol: str, interval: str = "1d", start_date: str = None, end_date: str = None) -> Optional[List[Dict]]:
        """è·å–è‚¡ç¥¨æˆ–ETFå†å²æ•°æ®ï¼ˆæ”¯æŒç¼“å­˜ï¼‰"""
        try:
            # å¤„ç†è‚¡ç¥¨ä»£ç æ ¼å¼ï¼šå…ˆå»æ‰åç¼€ï¼Œå†å»æ‰å‰ç¼€
            stock_code = symbol.split('.')[0]  # å»æ‰åç¼€ï¼ˆå¦‚.SS, .SZ, .HKï¼‰
            # å»æ‰å‰ç¼€ï¼ˆå¦‚sh, szï¼‰
            if stock_code.startswith('sh') or stock_code.startswith('sz'):
                stock_code = stock_code[2:]
            elif stock_code.startswith('SSE') or stock_code.startswith('SZSE'):
                stock_code = stock_code[3:]
            logger.info(f"æ­£åœ¨å¤„ç†è‚¡ç¥¨ä»£ç : {symbol}ï¼Œå¤„ç†å: {stock_code}")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºETFï¼ˆä»¥51ã€15ã€58å¼€å¤´çš„6ä½æ•°å­—ï¼‰
            is_etf = False
            if len(stock_code) == 6 and stock_code.isdigit():
                if stock_code.startswith(('51', '15', '58')):
                    is_etf = True
                    logger.info(f"è¯†åˆ«ä¸ºETF: {stock_code}")
            
            # è®¾ç½®ç¼“å­˜ç›®å½•å’Œæ–‡ä»¶å
            cache_dir = r"D:\code\financial_assistant_agent\cache\stock_data"
            cache_file = os.path.join(cache_dir, f"{symbol}_{interval}.json")
            logger.info(f"ç¼“å­˜ç›®å½•: {cache_dir}ï¼Œç¼“å­˜æ–‡ä»¶: {cache_file}")
            
            # åˆ›å»ºç¼“å­˜ç›®å½•
            if not os.path.exists(cache_dir):
                os.makedirs(cache_dir)
                logger.info(f"åˆ›å»ºç¼“å­˜ç›®å½•: {cache_dir}")
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆ7å¤©æœ‰æ•ˆæœŸï¼‰
            cache_valid = False
            if os.path.exists(cache_file):
                cache_time = os.path.getmtime(cache_file)
                if time.time() - cache_time < 7 * 24 * 3600:
                    cache_valid = True
                    logger.info(f"ç¼“å­˜æœ‰æ•ˆï¼Œå°†åŠ è½½ç¼“å­˜æ•°æ®")
            
            # åŠ è½½ç¼“å­˜æ•°æ®
            cached_data = []
            if cache_valid:
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        loaded_data = json.load(f)
                    
                    # å¤„ç†ä¸åŒçš„ç¼“å­˜æ ¼å¼
                    data_list = []
                    if isinstance(loaded_data, list):
                        data_list = loaded_data
                    elif isinstance(loaded_data, dict) and 'data' in loaded_data:
                        data_list = loaded_data['data']
                    else:
                        logger.error(f"ç¼“å­˜æ•°æ®æ ¼å¼é”™è¯¯: {type(loaded_data)}")
                        cache_valid = False
                        data_list = []
                    
                    # æ˜ å°„ä¸­æ–‡é”®ååˆ°è‹±æ–‡é”®å
                    field_mapping = {
                        'æ—¥æœŸ': 'date',
                        'å¼€ç›˜': 'open',
                        'æ”¶ç›˜': 'close',
                        'æœ€é«˜': 'high',
                        'æœ€ä½': 'low',
                        'æˆäº¤é‡': 'volume',
                        'æˆäº¤é¢': 'amount',
                        'æ¶¨è·Œå¹…': 'change',
                        'è‚¡ç¥¨ä»£ç ': 'stock_code'
                    }
                    
                    # è½¬æ¢æ•°æ®æ ¼å¼
                    for item in data_list:
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯è‹±æ–‡é”®åæ ¼å¼
                        if 'date' in item:
                            cached_data.append(item)
                        else:
                            # è½¬æ¢ä¸­æ–‡é”®åä¸ºè‹±æ–‡
                            new_item = {}
                            for cn_key, en_key in field_mapping.items():
                                if cn_key in item:
                                    new_item[en_key] = item[cn_key]
                            # æ·»åŠ symbolå­—æ®µ
                            new_item['symbol'] = symbol
                            cached_data.append(new_item)
                    
                    logger.info(f"ä»ç¼“å­˜åŠ è½½äº† {len(cached_data)} æ¡å†å²æ•°æ®")
                    
                    # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œè¿‡æ»¤æ•°æ®
                    if start_date and end_date:
                        filtered_data = []
                        for item in cached_data:
                            if 'date' in item and start_date <= item['date'] <= end_date:
                                filtered_data.append(item)
                        logger.info(f"æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤åå‰©ä½™ {len(filtered_data)} æ¡æ•°æ®")
                        return filtered_data
                    elif start_date:
                        filtered_data = []
                        for item in cached_data:
                            if 'date' in item and item['date'] >= start_date:
                                filtered_data.append(item)
                        logger.info(f"æŒ‰èµ·å§‹æ—¥æœŸè¿‡æ»¤åå‰©ä½™ {len(filtered_data)} æ¡æ•°æ®")
                        return filtered_data
                    elif end_date:
                        filtered_data = []
                        for item in cached_data:
                            if 'date' in item and item['date'] <= end_date:
                                filtered_data.append(item)
                        logger.info(f"æŒ‰ç»“æŸæ—¥æœŸè¿‡æ»¤åå‰©ä½™ {len(filtered_data)} æ¡æ•°æ®")
                        return filtered_data
                    return cached_data
                except Exception as e:
                    logger.error(f"åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
                    cache_valid = False
            
            # ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œä»APIè·å–æ–°æ•°æ®
            logger.info(f"ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œå¼€å§‹ä»APIè·å–æ–°æ•°æ®")
            
            # AkShareå‘¨æœŸæ˜ å°„
            period_map = {
                "1d": "daily",
                "1w": "weekly",
                "1mo": "monthly"
            }
            ak_period = period_map.get(interval, "daily")
            logger.info(f"æ—¶é—´å‘¨æœŸæ˜ å°„: {interval} -> {ak_period}")
            
            # è·å–å†å²æ•°æ®
            data = None
            if is_etf:
                # ETFæ•°æ®
                logger.info(f"æ­£åœ¨è·å–ETF {stock_code} çš„å†å²æ•°æ®")
                try:
                    if start_date and end_date:
                        data = ak.fund_etf_hist_em(symbol=stock_code, period=ak_period, start_date=start_date, end_date=end_date)
                    elif not start_date and not end_date:
                        # æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œè·å–æ‰€æœ‰å†å²æ•°æ®
                        data = ak.fund_etf_hist_em(symbol=stock_code, period=ak_period)
                    logger.info(f"ETFæ•°æ®åˆ—å: {data.columns.tolist()}")
                except Exception as e:
                    logger.error(f"è·å–ETFæ•°æ®å¤±è´¥: {e}")
            else:
                # è‚¡ç¥¨æ•°æ®
                logger.info(f"æ­£åœ¨è·å–è‚¡ç¥¨ {stock_code} çš„å†å²æ•°æ®")
                try:
                    if symbol.endswith('.SS') or symbol.endswith('.SZ'):
                        # Aè‚¡æ•°æ®
                        logger.info(f"è·å–Aè‚¡ {stock_code} çš„ {ak_period} æ•°æ®")
                        if start_date and end_date:
                            data = ak.stock_zh_a_hist(symbol=stock_code, period=ak_period, start_date=start_date, end_date=end_date)
                        else:
                            # æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œè·å–æ‰€æœ‰å†å²æ•°æ®
                            data = ak.stock_zh_a_hist(symbol=stock_code, period=ak_period)
                        logger.info(f"Aè‚¡æ•°æ®åˆ—å: {data.columns.tolist()}")
                        logger.info(f"Aè‚¡æ•°æ®å‰5è¡Œ: {data.head()}")
                    elif symbol.endswith('.HK'):
                        # æ¸¯è‚¡æ•°æ®
                        logger.info(f"è·å–æ¸¯è‚¡ {stock_code} çš„å†å²æ•°æ®")
                        try:
                            if start_date and end_date:
                                data = ak.stock_hk_hist(symbol=stock_code, period=ak_period, start_date=start_date, end_date=end_date)
                            else:
                                # æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œè·å–æ‰€æœ‰å†å²æ•°æ®
                                data = ak.stock_hk_hist(symbol=stock_code, period=ak_period)
                            logger.info(f"æ¸¯è‚¡æ•°æ®åˆ—å: {data.columns.tolist()}")
                        except Exception as e:
                            logger.error(f"æ¸¯è‚¡APIè°ƒç”¨å¤±è´¥: {e}")
                            return []
                except Exception as e:
                    logger.error(f"è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
            
            logger.info(f"è·å–åˆ°æ•°æ®å½¢çŠ¶: {data.shape if data is not None else 'None'}")
            
            if data is not None and not data.empty:
                # è½¬æ¢æ•°æ®æ ¼å¼
                historical_data = []
                
                # ETFæ•°æ®å­—æ®µæ˜ å°„ - é€‚é…ä¸åŒçš„å­—æ®µå
                if is_etf:
                    for _, row in data.iterrows():
                        # å°è¯•å¤šç§å­—æ®µåç»„åˆ
                        open_col = row.get('å¼€ç›˜ä»·', row.get('å¼€ç›˜', 0))
                        high_col = row.get('æœ€é«˜ä»·', row.get('æœ€é«˜', 0))
                        low_col = row.get('æœ€ä½ä»·', row.get('æœ€ä½', 0))
                        close_col = row.get('æ”¶ç›˜ä»·', row.get('æ”¶ç›˜', 0))
                        volume_col = row.get('æˆäº¤é‡', row.get('æˆäº¤', 0))
                        amount_col = row.get('æˆäº¤é¢', row.get('é‡‘é¢', 0))
                        change_col = row.get('æ¶¨è·Œå¹…', row.get('æ¶¨è·Œ', 0))
                        
                        # å¤„ç†æ—¥æœŸå­—æ®µ
                        date_value = row.get('æ—¥æœŸ', '')
                        if hasattr(date_value, 'strftime'):
                            date_str = date_value.strftime('%Y-%m-%d')
                        else:
                            date_str = str(date_value)
                        
                        historical_data.append({
                            'date': date_str,
                            'open': float(open_col),
                            'high': float(high_col),
                            'low': float(low_col),
                            'close': float(close_col),
                            'volume': int(volume_col) if volume_col != '-' else 0,
                            'amount': float(amount_col) if amount_col != '-' else 0,
                            'change': float(change_col) if change_col != '-' else 0,
                            'symbol': symbol
                        })
                # è‚¡ç¥¨æ•°æ®å­—æ®µæ˜ å°„
                else:
                    for _, row in data.iterrows():
                        # å¤„ç†æ—¥æœŸå­—æ®µ
                        date_value = row.get('æ—¥æœŸ', '')
                        if hasattr(date_value, 'strftime'):
                            date_str = date_value.strftime('%Y-%m-%d')
                        else:
                            date_str = str(date_value)
                        
                        historical_data.append({
                            'date': date_str,
                            'open': float(row.get('å¼€ç›˜', 0)),
                            'high': float(row.get('æœ€é«˜', 0)),
                            'low': float(row.get('æœ€ä½', 0)),
                            'close': float(row.get('æ”¶ç›˜', 0)),
                            'volume': int(row.get('æˆäº¤é‡', 0)),
                            'amount': float(row.get('æˆäº¤é¢', 0)),
                            'change': float(row.get('æ¶¨è·Œå¹…', 0)),
                            'symbol': symbol
                        })
                
                logger.info(f"è½¬æ¢åè·å–åˆ° {len(historical_data)} æ¡å†å²æ•°æ®")
                
                # ä¿å­˜åˆ°ç¼“å­˜
                try:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(historical_data, f, ensure_ascii=False, indent=2)
                    logger.info(f"å†å²æ•°æ®å·²ä¿å­˜åˆ°ç¼“å­˜: {cache_file}")
                except Exception as e:
                    logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
                
                return historical_data
            else:
                logger.warning(f"æœªè·å–åˆ°å†å²æ•°æ®: {symbol}")
                return []
            
        except Exception as e:
            logger.error(f"è·å–å†å²æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return None
    
    def generate_enhanced_response(self, query: str, history: List[tuple] = None, 
                                  data: Dict = None, intent_analysis: Dict = None) -> Dict:
        """ç”Ÿæˆå¢å¼ºçš„è‡ªç„¶è¯­è¨€å›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            history: å¯¹è¯å†å²
            data: æ•°æ®ä¸Šä¸‹æ–‡ï¼ˆå¦‚è‚¡ç¥¨ä»·æ ¼ã€å¸‚åœºæŒ‡æ•°ç­‰ï¼‰
            intent_analysis: æ„å›¾åˆ†æç»“æœ
            
        Returns:
            Dict: åŒ…å«ç”Ÿæˆçš„å›ç­”å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # å‡†å¤‡æ•°æ®ä¸Šä¸‹æ–‡
            data_sources = {
                'real_time_data': data or {}
            }
            
            # ä½¿ç”¨PromptEngineæ„å»ºæç¤ºè¯
            prompt = self.prompt_engine.construct_prompt(
                query=query,
                history=history or [],
                data_sources=data_sources,
                intent_analysis=intent_analysis or {}
            )
            
            # ä½¿ç”¨RAGç”Ÿæˆå¢å¼ºå›ç­”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            rag_result = self.prompt_engine.generate_with_rag(
                query=query,
                history=history or [],
                data_sources=data_sources,
                intent_analysis=intent_analysis or {}
            )
            
            # åˆå¹¶ç»“æœ
            result = {
                'prompt': rag_result.get('prompt', prompt),
                'has_rag_context': rag_result.get('has_rag_context', False)
            }
            
            # å¦‚æœæœ‰RAGæºæ–‡æ¡£ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if rag_result.get('source_documents'):
                result['source_documents'] = rag_result['source_documents']
            
            return result
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¢å¼ºå›ç­”å¤±è´¥: {str(e)}")
            return {
                'error': str(e),
                'prompt': None,
                'has_rag_context': False
            }


# è¾…åŠ©å‡½æ•°éƒ¨åˆ†
def add_source_citations(response: str, sources: List[Dict], kb) -> str:
    """ä¸ºå›ç­”æ·»åŠ æ¥æºå¼•ç”¨æ ‡æ³¨"""
    if not sources:
        return response

    # æå–æ¥æºä¿¡æ¯
    source_info = []
    for i, source in enumerate(sources, 1):
        # å¤„ç†çŸ¥è¯†åº“æ¥æº
        if hasattr(source, 'get') and source.get('document_id'):
            doc_id = source.get("document_id")
            url = kb.get_url_from_doc_id(doc_id) if hasattr(kb, 'get_url_from_doc_id') else f"æ–‡æ¡£ID: {doc_id}"
            source_info.append(f"[{i}] çŸ¥è¯†åº“: {url}")
        
        # å¤„ç†APIæ•°æ®æ¥æº
        elif hasattr(source, 'get') and source.get('source'):
            source_type = source.get('source', 'æœªçŸ¥æ¥æº')
            timestamp = source.get('timestamp', '')
            source_info.append(f"[{i}] {source_type}æ•°æ®: {timestamp}")

    # æ·»åŠ å¼•ç”¨æ ‡æ³¨
    if source_info:
        citation_text = "\n\nğŸ“š æ•°æ®æ¥æº:\n" + "\n".join(source_info)
        return response + citation_text
    
    return response


# æ–°å¢å·¥å…·å‡½æ•°
def validate_api_keys(api_keys: Dict) -> Dict:
    """éªŒè¯APIå¯†é’¥æœ‰æ•ˆæ€§"""
    valid_keys = {}
    
    for api_name, config in api_keys.items():
        if config.get('api_key') and config.get('api_key') != 'your_api_key_here':
            valid_keys[api_name] = config
        else:
            logger.warning(f"{api_name} APIå¯†é’¥æœªé…ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼")
    
    return valid_keys


def format_financial_data(data: Dict, data_type: str) -> str:
    """æ ¼å¼åŒ–é‡‘èæ•°æ®ç”¨äºæ˜¾ç¤º"""
    if not data:
        return "æš‚æ— æ•°æ®"
    
    try:
        if data_type == "stock":
            return f"""ğŸ“Š è‚¡ç¥¨æ•°æ®: {data.get('symbol', 'N/A')}
ğŸ’° å½“å‰ä»·æ ¼: {data.get('price', 'N/A')} 
ğŸ“ˆ æ¶¨è·Œå¹…: {data.get('change', 'N/A')}%
ğŸ“Š æˆäº¤é‡: {data.get('volume', 'N/A')}
â° æ›´æ–°æ—¶é—´: {data.get('timestamp', 'N/A')}"""
        
        elif data_type == "market":
            return f"""ğŸ¦ å¸‚åœºæ¦‚å†µ
ä¸»è¦æŒ‡æ•°è¡¨ç°:
{chr(10).join([f"- {name}: {info.get('price', 'N/A')} ({info.get('change', 'N/A')}%)" 
               for name, info in data.get('major_indices', {}).items()])}"""
        
        elif data_type == "news":
            articles = data.get('articles', [])
            return f"""ğŸ“° æœ€æ–°è´¢ç»æ–°é—» ({len(articles)}æ¡)
{chr(10).join([f'{i+1}. {article.get("title", "N/A")}' 
               for i, article in enumerate(articles[:3])])}"""
    
    except Exception as e:
        logger.error(f"æ ¼å¼åŒ–æ•°æ®å¤±è´¥: {e}")
    
    return "æ•°æ®æ ¼å¼å¼‚å¸¸"


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•AkShareåŠŸèƒ½
    api = FinancialDataAPI()
    
    # æµ‹è¯•Aè‚¡æ•°æ®
    stock_data = api.get_stock_price("600519.SS")
    print("è´µå·èŒ…å°æ•°æ®:", stock_data)
    
    # æµ‹è¯•æŒ‡æ•°æ•°æ®
    index_data = api.get_market_index("ä¸Šè¯æŒ‡æ•°")
    print("ä¸Šè¯æŒ‡æ•°æ•°æ®:", index_data)
    
    # æµ‹è¯•æ–°é—»æ•°æ®
    news_data = api.get_financial_news("è´¢ç»", 5)
    print("è´¢ç»æ–°é—»:", news_data)
    
    # æµ‹è¯•ç”Ÿæˆå¢å¼ºå›ç­”åŠŸèƒ½
    print("\n=== æµ‹è¯•å¢å¼ºå›ç­”åŠŸèƒ½ ===")
    query = "è´µå·èŒ…å°çš„æœ€æ–°ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ"
    history = [
        ("ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€äº›è‚¡ç¥¨ä¿¡æ¯", "ä½ å¥½ï¼æˆ‘æ˜¯é‡‘èåŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ")
    ]
    
    # ç”Ÿæˆå¢å¼ºå›ç­”
    enhanced_response = api.generate_enhanced_response(
        query=query,
        history=history,
        data={"stock_price": stock_data},
        intent_analysis={"primary_intent": "specific_stock"}
    )
    
    print(f"ç”Ÿæˆçš„æç¤ºè¯:\n{enhanced_response['prompt']}")
    print(f"æ˜¯å¦ä½¿ç”¨RAG: {enhanced_response['has_rag_context']}")